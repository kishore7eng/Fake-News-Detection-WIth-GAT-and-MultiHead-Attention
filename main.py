# -*- coding: utf-8 -*-

"""Lets see.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RH6vLCfhx2LtphzDgV_8_CDp3L0YvnYp
    
"""

##################################### FAKE NEWS DETECTION USING GRAPH NEURAL NETWORK AND IMPROVEMENT WITH GRAPH ATTENTION NETWORK WITH MULTIHEAD ATTENTION ############################################

import sys

import torch
vers = torch.__version__
print("Torch vers: ", vers)

# PyG installation
# !pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html
# !pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html
# !pip install -q git+https://github.com/rusty1s/pytorch_geometric.git

import torch_geometric

from torch_geometric.datasets import UPFD
"""## Inbuilt Dataset

- Contains news propagation graphs extracted from Twitter
- feature="content" --> Spacy Word2Vec + Profile features
- feature="bert" --> BERT + Profile features

"""


# total arguments
n = len(sys.argv)

cmd_feature = ""
cmd_name =""

available_features = ['content', 'bert']
available_dataset = ['gossipcop','politifact']

if n != 3:
	sys.exit("Number of Arguments not correct!! HINT: After program name, enter dataset_name(gossipcop/politifact) and model_name(bert/content)")
	
else:
	cmd_name = sys.argv[1]
	cmd_feature = sys.argv[2]
	
	if cmd_name not in available_dataset:
		sys.exit("Name of dataset should be gossipcop or politifact ONLY...")
		
	if cmd_feature not in available_features:
		sys.exit("Name of Feature should be content(for spaCy word2vec) or bert(for BERT) ONLY...")


train_data = UPFD(root=".", name=cmd_name, feature=cmd_feature, split="train")
test_data = UPFD(root=".", name=cmd_name, feature=cmd_feature, split="test")

#train_data = UPFD(root=".", name="gossipcop", feature="bert", split="train")
#test_data = UPFD(root=".", name="gossipcop", feature="bert", split="test")

#train_data = UPFD(root=".", name="politifact", feature="content", split="train")
#test_data = UPFD(root=".", name="politifact", feature="content", split="test")

#train_data = UPFD(root=".", name="politifact", feature="bert", split="train")
#test_data = UPFD(root=".", name="politifact", feature="bert", split="test")


print("Train Samples: ", len(train_data))
print("Test Samples: ", len(test_data))

"""### Class distribution"""

import pandas as pd
labels = [data.y.item() for i, data in enumerate(train_data)]
df = pd.DataFrame(labels, columns=["Labels"])
df["Labels"].hist()

"""### Data Loaders"""

from torch_geometric.loader import DataLoader
train_loader = DataLoader(train_data, batch_size=128, shuffle=True)
test_loader = DataLoader(test_data, batch_size=128, shuffle=False)

## Model and Training 

from torch_geometric.nn import global_max_pool as gmp
from torch_geometric.nn import GCNConv
from torch.nn import Linear

print('GNN using GCN')

class GNN(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        
        # Graph Convolutions
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.conv3 = GCNConv(hidden_channels, hidden_channels)

        # Readout
        self.lin_news = Linear(in_channels, hidden_channels)
        self.lin0 = Linear(hidden_channels, hidden_channels)
        self.lin1 = Linear(2*hidden_channels, out_channels)

    def forward(self, x, edge_index, batch):
        # Graph Convolutions
        h = self.conv1(x, edge_index).relu()
        h = self.conv2(h, edge_index).relu()
        h = self.conv3(h, edge_index).relu()

        # Pooling
        h = gmp(h, batch)

        # Readout
        h = self.lin0(h).relu()

        # ENDOGENEOUS PREFERENCE

        # According to UPFD paper: Include raw word2vec embeddings of news 
        # This is done per graph in the batch
        root = (batch[1:] - batch[:-1]).nonzero(as_tuple=False).view(-1)
        root = torch.cat([root.new_zeros(1), root + 1], dim=0)
        news = x[root]
        news = self.lin_news(news).relu()
        
        out = self.lin1(torch.cat([h, news], dim=-1)) # CONCAT News Textual Embedding and User Engagement Embedding
        return torch.sigmoid(out)

GNN(train_data.num_features, 128, 1)



from sklearn.metrics import accuracy_score, f1_score


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = GNN(train_data.num_features, 128, 1).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)
loss_fnc = torch.nn.BCELoss()


optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)

def train(epoch):

        model.train()
        total_loss = 0
        for data in train_loader:
            data = data.to(device)
            optimizer.zero_grad()
            out = model(data.x, data.edge_index, data.batch)
            loss = loss_fnc(torch.reshape(out, (-1,)), data.y.float())
            loss.backward()
            optimizer.step()
            total_loss += float(loss) * data.num_graphs
        return total_loss / len(train_loader.dataset)


@torch.no_grad()
def test(epoch):

        model.eval()
        total_loss = 0
        all_preds = []
        all_labels = []
        for data in test_loader:
            data = data.to(device)
            out = model(data.x, data.edge_index, data.batch)
            loss = loss_fnc(torch.reshape(out, (-1,)), data.y.float())
            total_loss += float(loss) * data.num_graphs
            all_preds.append(torch.reshape(out, (-1,)))
            all_labels.append(data.y.float())

        # Calculate Metrics

        accuracy, f1 = metrics(all_preds, all_labels)

        return total_loss / len(test_loader.dataset), accuracy, f1

def metrics(preds, gts):
    preds = torch.round(torch.cat(preds))
    gts = torch.cat(gts)
    acc = accuracy_score(preds, gts)
    f1 = f1_score(preds, gts)
    return acc, f1

last_20_acc_avg = 0;
for epoch in range(40):
    train_loss = train(epoch)
    test_loss, test_acc, test_f1 = test(epoch)
    if(epoch >= 20):
      last_20_acc_avg += test_acc
    print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.2f} | '
          f'TestLoss: {test_loss:.2f} | TestAcc: {test_acc:.2f} | TestF1: {test_f1:.2f}')
    
print(f'Average test accuracy (for last 20 epochs): {last_20_acc_avg/20}')



print('GAT and Multihead Attention [Improvement over GCN]')

"""## GAT and Multihead Attention [Improvement over GCN]"""

from torch_geometric.nn import GATv2Conv
import torch.nn.functional as F

class GAT(torch.nn.Module):
  """Graph Attention Network"""
  def __init__(self, dim_in, dim_h, dim_out, heads=4):
    super().__init__()
    self.gat1 = GATv2Conv(dim_in, dim_h, heads=heads)
    self.gat2 = GATv2Conv(dim_h*heads, dim_h, heads=heads)
    self.gat3 = GATv2Conv(dim_h*heads, dim_h, heads=1)

    self.optimizer = torch.optim.Adam(self.parameters(),
                                      lr=0.005,
                                      weight_decay=5e-4)
    
    # Readout
    self.lin_news = Linear(dim_in, dim_h)
    self.lin0 = Linear(dim_h, dim_h)
    self.lin1 = Linear(2*dim_h, dim_out)

  def forward(self, x, edge_index,batch):
    #h = F.dropout(x, p=0.6, training=self.training)
    h = self.gat1(x, edge_index).relu()
    #h = F.relu(h)
    #h = F.dropout(h, p=0.6, training=self.training)
    h = self.gat2(h, edge_index).relu()
    #h = F.relu(h)
    h = self.gat3(h,edge_index).relu()
    #h = F.relu(h)

    #print(h.shape)


    #Pooling
    h = gmp(h, batch)

    #print(h.shape)

    # Readout
    h = self.lin0(h).relu()

    # ENDOGENEOUS PREFERENCE
    
    # According to UPFD paper: Include raw word2vec embeddings of news 
    # This is done per graph in the batch
    root = (batch[1:] - batch[:-1]).nonzero(as_tuple=False).view(-1)
    root = torch.cat([root.new_zeros(1), root + 1], dim=0)
    # root is e.g. [   0,   14,   94,  171,  230,  302, ... ]
    news = x[root]
    news = self.lin_news(news).relu()
        
    out = self.lin1(torch.cat([h, news], dim=-1)) # CONCAT News Textual Embedding and User Engagement Embedding
    return torch.sigmoid(out)
    #return h, F.log_softmax(h, dim=1)

GAT(train_data.num_features, 128, 4)

from sklearn.metrics import accuracy_score, f1_score

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model2 = GAT(train_data.num_features, 128, 1, 4).to(device)
optimizer = torch.optim.Adam(model2.parameters(), lr=0.01, weight_decay=0.01)
loss_fnc = torch.nn.BCELoss()


optimizer = torch.optim.Adam(model2.parameters(), lr=0.005, weight_decay=5e-4)

def train2(epoch):

        model2.train()
        total_loss = 0
        for data in train_loader:
            data = data.to(device)
            optimizer.zero_grad()
            out = model2(data.x, data.edge_index, data.batch)
            loss = loss_fnc(torch.reshape(out, (-1,)), data.y.float())
            loss.backward()
            optimizer.step()
            total_loss += float(loss) * data.num_graphs
        return total_loss / len(train_loader.dataset)


@torch.no_grad()
def test2(epoch):

        model2.eval()
        total_loss = 0
        all_preds = []
        all_labels = []
        for data in test_loader:
            data = data.to(device)
            out = model2(data.x, data.edge_index, data.batch)
            loss = loss_fnc(torch.reshape(out, (-1,)), data.y.float())
            total_loss += float(loss) * data.num_graphs
            all_preds.append(torch.reshape(out, (-1,)))
            all_labels.append(data.y.float())

        # Calculate Metrics
        accuracy, f1 = metrics2(all_preds, all_labels)

        return total_loss / len(test_loader.dataset), accuracy, f1

def metrics2(preds, gts):
    preds = torch.round(torch.cat(preds))
    gts = torch.cat(gts)
    acc = accuracy_score(preds, gts)
    f1 = f1_score(preds, gts)
    return acc, f1

last_20_acc_avg = 0;
for epoch in range(40):
    train_loss = train2(epoch)
    test_loss, test_acc, test_f1 = test2(epoch)
    if(epoch >= 20):
      last_20_acc_avg += test_acc
    print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.2f} | '
          f'TestLoss: {test_loss:.2f} | TestAcc: {test_acc:.2f} | TestF1: {test_f1:.2f}')
    
print(f'Average test accuracy (for last 20 epochs): {last_20_acc_avg/20}')




